## Understading LLM, GPT and Neural Nets From Scratch
This repository contains Jupyter notebooks created while coding along with Andrej Karpathy's `Neural Networks: Zero to Hero` lecture series.  

Following are covered in the lectures:  
* Backpropagation coded from scratch, `No Autograd`. Intution developed from calculus chain rule and coded in real time
* `Batch normalization` coded and dsicussed in detail
* Neural Net (NN) `training dynamics` discussed in detail
* Importance of weight matrix initialization in NN training
* Activation functions, their graphs and derivates and how they effect NN training
* `Debugging neural net` training issues using various layer level graph plots. (aka Diagnostic tools)
* Explained and developed `Decoder Only Transformer Block` and used it to develop GPT. Also explained Encoder-Decoder attention blocks
* Briefly touched upon post training processes related to LLM (GPT here)

## My Appraoch
As opposed to just watching them and taking hand written notes, I coded along which made me think a lot more on what was being taught.  

My notebooks here are slightly different from the lectures in following ways:
* I have `retained experimental cells`, which he used for explaining concepts and later deleted to avoid clutter, so that it easier to understand later on.
* I have noted `important comments` made by him while explaining some concepts

While watching these lectures, I had lot of doubts and concerns regarding various comments made by him and many concepts that were introduced in them. To make better sense of them inside my brain, I had to engaged in deep discussion session with ChatGPT which were really insightful and helped a lot in understanding lecture content a lot better.  

I have shared the snapshots of those chats below for future reference for myself or anyone who may find them useful.


